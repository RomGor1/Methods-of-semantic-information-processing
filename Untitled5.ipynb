{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP44LOlF13NESffYpmn43Y9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomGor1/Methods-of-semantic-information-processing/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "00FblsZ_kVzK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleGPT:\n",
        "    def __init__(self, vocab_size, embedding_dim, context_length):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_length = context_length\n",
        "\n",
        "        # Инициализация параметров\n",
        "        self.token_embedding = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
        "        self.position_embedding = np.random.randn(context_length, embedding_dim) * 0.01\n",
        "        self.lm_head = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, probs, targets):\n",
        "        m = targets.shape[0]\n",
        "        log_probs = -np.log(probs[np.arange(m), targets])\n",
        "        return np.sum(log_probs) / m\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        # Token and position embeddings\n",
        "        tok_emb = self.token_embedding[inputs]  # (batch_size, context_length, embedding_dim)\n",
        "        pos_emb = self.position_embedding[np.arange(self.context_length)]  # (context_length, embedding_dim)\n",
        "        x = tok_emb + pos_emb  # (batch_size, context_length, embedding_dim)\n",
        "\n",
        "        # Linear projection\n",
        "        logits = np.dot(x, self.lm_head)  # (batch_size, context_length, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Calculate loss\n",
        "            logits_flat = logits.reshape(-1, self.vocab_size)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            probs = self.softmax(logits_flat)\n",
        "            loss = self.cross_entropy_loss(probs, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def backward(self, inputs, targets, learning_rate):\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        # Forward pass\n",
        "        logits, loss = self.forward(inputs, targets)\n",
        "\n",
        "        # Gradient of softmax + cross entropy\n",
        "        logits_flat = logits.reshape(-1, self.vocab_size)\n",
        "        targets_flat = targets.reshape(-1)\n",
        "        probs = self.softmax(logits_flat)\n",
        "\n",
        "        d_logits_flat = probs.copy()\n",
        "        d_logits_flat[np.arange(len(targets_flat)), targets_flat] -= 1\n",
        "        d_logits_flat /= batch_size\n",
        "        d_logits = d_logits_flat.reshape(batch_size, self.context_length, self.vocab_size)\n",
        "\n",
        "        # Gradient for lm_head\n",
        "        x = self.token_embedding[inputs] + self.position_embedding[np.arange(self.context_length)]\n",
        "        d_lm_head = np.zeros_like(self.lm_head)\n",
        "\n",
        "        # Manually compute the gradient for lm_head to avoid dimension issues\n",
        "        for b in range(batch_size):\n",
        "            for t in range(self.context_length):\n",
        "                d_lm_head += np.outer(x[b, t], d_logits[b, t])\n",
        "\n",
        "        # Gradient for embeddings\n",
        "        d_x = np.dot(d_logits, self.lm_head.T)\n",
        "        d_token_embedding = np.zeros_like(self.token_embedding)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for t in range(self.context_length):\n",
        "                d_token_embedding[inputs[b, t]] += d_x[b, t]\n",
        "\n",
        "        d_position_embedding = d_x.sum(axis=0)\n",
        "\n",
        "        # Update parameters\n",
        "        self.lm_head -= learning_rate * d_lm_head\n",
        "        self.token_embedding -= learning_rate * d_token_embedding\n",
        "        self.position_embedding -= learning_rate * d_position_embedding\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self, inputs, targets, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            loss = self.backward(inputs, targets, learning_rate)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[-self.context_length:]\n",
        "            logits, _ = self.forward(idx_cond[np.newaxis, :])\n",
        "            logits = logits[0, -1, :]\n",
        "            probs = self.softmax(logits)\n",
        "            idx_next = np.random.choice(self.vocab_size, p=probs)\n",
        "            idx = np.append(idx, idx_next)\n",
        "        return idx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"hello world hello hello world\"\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "icteW-43oI9R"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mappings\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "98TFreFVoSyl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "data = np.array(encode(text))\n",
        "context_length = 3\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data) - context_length):\n",
        "    X.append(data[i:i+context_length])\n",
        "    y.append(data[i+context_length])\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "rcSHTx5_oXvQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train model\n",
        "gpt = SimpleGPT(vocab_size=vocab_size, embedding_dim=8, context_length=context_length)\n",
        "gpt.train(X, y, epochs=1000, learning_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXdRMGM7ocUq",
        "outputId": "47093c17-5277-4152-84a8-0d57ee0f4129"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.0795\n",
            "Epoch 100, Loss: 1.9875\n",
            "Epoch 200, Loss: inf\n",
            "Epoch 300, Loss: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-d4daf2e92082>:20: RuntimeWarning: divide by zero encountered in log\n",
            "  log_probs = -np.log(probs[np.arange(m), targets])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 400, Loss: inf\n",
            "Epoch 500, Loss: inf\n",
            "Epoch 600, Loss: inf\n",
            "Epoch 700, Loss: inf\n",
            "Epoch 800, Loss: inf\n",
            "Epoch 900, Loss: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "print(\"\\nGenerated text:\")\n",
        "start = np.array(encode(\"hel\"))[:context_length]  # Ensure correct length\n",
        "generated = gpt.generate(start, max_new_tokens=10)\n",
        "print(decode(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqleGWwTofqc",
        "outputId": "07e008eb-19c2-4264-9d06-831f7f4ba934"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "hel          \n"
          ]
        }
      ]
    }
  ]
}