# Импорт необходимых библиотек
from natasha import MorphVocab, Doc, Segmenter, NewsEmbedding, NewsMorphTagger

# Инициализация сегментатора, морфологического теггера и морфологического словаря
segmenter = Segmenter()
morph_vocab = MorphVocab()

# Загрузка модели для морфологического анализа
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)

# 1. Лемматизация и стемминг с использованием Natasha
def lemmatize_and_stem(text):
    doc = Doc(text)
    doc.segment(segmenter)  # Сегментация текста на токены
    doc.tag_morph(morph_tagger)  # Морфологический разбор
    
    # Приведение токенов к леммам
    for token in doc.tokens:
        token.lemmatize(morph_vocab)
    
    lemmatized_text = [token.lemma for token in doc.tokens]
    stemmed_text = [token.lemma for token in doc.tokens]  # В Natasha стемминг не поддерживается напрямую, используем леммы
    
    return lemmatized_text, stemmed_text

# Пример текста
text = "Мама мыла раму, а потом пошла гулять."

# Лемматизация и стемминг
lemmatized_text, stemmed_text = lemmatize_and_stem(text)
print("Лемматизация:", lemmatized_text)
print("Стемминг:", stemmed_text)

# 2. Функция для токенизации всех символов из ASCII
def tokenize_ascii(text):
    return list(text)

# Пример использования
ascii_text = "Hello, World!"
tokens = tokenize_ascii(ascii_text)
print("Токенизация ASCII:", tokens)

# 3. Функция для векторизации всех символов из ASCII
def vectorize_ascii(text):
    return [ord(char) for char in text]

# Пример использования
ascii_text = "Hello, World!"
vectors = vectorize_ascii(ascii_text)
print("Векторизация ASCII:", vectors)

# 4. Токенизация и векторизация текста после лемматизации и стемминга
processed_text = ' '.join(lemmatized_text)

# Токенизация
tokens = tokenize_ascii(processed_text)
print("Токенизация после обработки:", tokens)

# Векторизация
vectors = vectorize_ascii(processed_text)
print("Векторизация после обработки:", vectors)
